{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT8JC38Bk0Wa"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install torch\n",
        "!pip install sklearn\n",
        "!pip install scipy\n",
        "!pip install tensorflow\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMx7rcdVtWgq"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PMmaVhugtTQy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.image as mpimg\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfJJmzX8tawG"
      },
      "source": [
        "CNN network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2MvEMY-GtSPl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Define the layers\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3,stride=1 , padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3,stride=1 , padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3,stride=1 , padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.conv4 = nn.Conv1d(128, 128, kernel_size=3,stride=1 , padding=1)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5 = nn.Conv1d(128, 256, kernel_size=3,stride=1 , padding=1)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.conv6 = nn.Conv1d(256, 256, kernel_size=3,stride=1 , padding=1)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.maxpool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv7 = nn.Conv1d(256, 512, kernel_size=3,stride=1 , padding=1)\n",
        "        self.bn7 = nn.BatchNorm1d(512)\n",
        "        self.relu7 = nn.ReLU()\n",
        "        self.conv8 = nn.Conv1d(512, 512, kernel_size=3,stride=1 , padding=1)\n",
        "        self.relu8 = nn.ReLU()\n",
        "        self.maxpool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(512 * 6, 512)\n",
        "        self.do1 = nn.Dropout(p=0.05)\n",
        "        self.bn9 = nn.BatchNorm1d(512)\n",
        "        self.relu9 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn10 = nn.BatchNorm1d(256)\n",
        "        self.relu10 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Perform forward pass\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.relu6(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = self.conv7(x)\n",
        "        x = self.relu7(x)\n",
        "        x = self.conv8(x)\n",
        "        x = self.relu8(x)\n",
        "        x = self.maxpool4(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.do1(x)\n",
        "        x = self.relu9(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu10(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Class:"
      ],
      "metadata": {
        "id": "t_T8DX5eoiWQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FRE4U-IHv86y"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x_train, y_train):\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_train)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x_train[idx]\n",
        "        y = self.y_train[idx]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Hyper parameters and train/test split:"
      ],
      "metadata": {
        "id": "D8G8MUfyokHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Cm0xCKexu5GU"
      },
      "outputs": [],
      "source": [
        "# Step 0: read data from Mat file\n",
        "file_path = r\"Dataset.mat\"\n",
        "mat = scipy.io.loadmat(file_path)\n",
        "x_values = mat['featuresMat']          # data, labels\n",
        "y_values = mat['positionMat']\n",
        "x_values = x_values.transpose()\n",
        "y_values = y_values.transpose()/1000\n",
        "\n",
        "# time to train our model\n",
        "# hyper-parameters\n",
        "batch_size = 1024\n",
        "learning_rate = 5e-5\n",
        "epochs = 500\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.15, shuffle=True)\n",
        "x_train = torch.tensor(x_train)\n",
        "y_train = torch.tensor(y_train)\n",
        "\n",
        "x_test = torch.tensor(x_test)\n",
        "y_test = torch.tensor(y_test)\n",
        "\n",
        "x_train = x_train.unsqueeze(1)\n",
        "x_test = x_test.unsqueeze(1)\n",
        "\n",
        "trainset = CustomDataset(x_train, y_train)\n",
        "testset = CustomDataset(x_test, y_test)\n",
        "\n",
        "# dataloaders - creating batches and shuffling the data\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# device - cpu or gpu?\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# loss criteria\n",
        "mse_criterion = nn.MSELoss()\n",
        "\n",
        "# build our model and send it to the device\n",
        "model = SimpleCNN().to(device) # no need for parameters as we alredy defined them in the class\n",
        "\n",
        "# optimizer - SGD, Adam, RMSProp...\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training:"
      ],
      "metadata": {
        "id": "uPtPrzCyonJw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoHCqUcewHZp"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "loss_history = []\n",
        "test_loss_history = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()  # Put the model in training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    mse_running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Send inputs and labels to the device\n",
        "        inputs = inputs.to(device, dtype=torch.float32)\n",
        "        labels = labels.to(device, dtype=torch.float32)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate MSE loss\n",
        "        mse_loss = mse_criterion(outputs, labels)\n",
        "\n",
        "        # Combine MSE loss according to your requirements\n",
        "        loss = mse_loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the running losses\n",
        "        running_loss += loss.item()\n",
        "        mse_running_loss += mse_loss.item()\n",
        "\n",
        "    # Calculate average training losses\n",
        "    avg_loss = running_loss / len(trainloader)\n",
        "    avg_mse_loss = mse_running_loss / len(trainloader)\n",
        "    loss_history.append(avg_mse_loss)\n",
        "    # Evaluate the model on the test set\n",
        "    model.eval()  # Put the model in evaluation mode\n",
        "    test_loss = 0.0\n",
        "    test_mse_loss = 0.0\n",
        "    test_mae_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "\n",
        "            inputs, labels = data\n",
        "\n",
        "            # Send inputs and labels to the device\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            labels = labels.to(device, dtype=torch.float32)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate MSE loss\n",
        "            mse_loss = mse_criterion(outputs, labels)\n",
        "\n",
        "            # Calculate MAE loss\n",
        "\n",
        "            # Combine MSE and MAE losses according to your requirements\n",
        "            loss = mse_loss\n",
        "\n",
        "            # Update the test losses\n",
        "            test_loss += loss.item()\n",
        "            test_mse_loss += mse_loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update total and correct predictions\n",
        "            total += labels.size(0)\n",
        "\n",
        "    # Calculate average test losses and accuracy\n",
        "    avg_test_loss = test_loss / len(testloader)\n",
        "    test_loss_history.append(avg_test_loss)\n",
        "    avg_test_mse_loss = test_mse_loss / len(testloader)\n",
        "\n",
        "    # Print the epoch number, training losses, test losses, and test accuracy\n",
        "    print(f\"Epoch: {epoch} | Training Loss: {avg_loss:.4f} (MSE: {avg_mse_loss:.4f}) | Test Loss: {avg_test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Model:"
      ],
      "metadata": {
        "id": "wmypjsE2obRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "mHkw6Jlimn2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotters:"
      ],
      "metadata": {
        "id": "2qNc2LR1odxW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cw-1hmA4s9Z"
      },
      "outputs": [],
      "source": [
        "cells = np.array([[381, 504, 65],\n",
        "         [428, -17, 514],\n",
        "         [244, -44, -597],\n",
        "         [401, -408, -447],\n",
        "         [229, -716, 233],\n",
        "         [485, -254, 907],\n",
        "         [318, 944, -257],\n",
        "         [324, -1070, -497],\n",
        "         [272, -357, -1145]])\n",
        "\n",
        "#Fig 1\n",
        "#--------\n",
        "#FIX\n",
        "loss = loss_history\n",
        "arr_epochs = np.arange(epochs)\n",
        "fig = plt.figure(1, figsize=(10, 10))\n",
        "fig.suptitle('CNN Analysis')\n",
        "ax1 = plt.subplot2grid((1, 1), (0, 0), colspan=1, rowspan=1)\n",
        "\n",
        "# Exclude the first few epochs so the graph is easier to read\n",
        "SKIP = 0\n",
        "ax1.plot(arr_epochs[SKIP:], loss[SKIP:], 'r-', label='Training loss')\n",
        "ax1.plot(arr_epochs[SKIP:], test_loss_history[SKIP:], 'g-', label='Test loss')\n",
        "ax1.set_title('Training loss')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, which='both', axis='both')\n",
        "############\n",
        "# Calculate and print the loss on our test dataset\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Send inputs and labels to the device\n",
        "        inputs = inputs.to(device, dtype=torch.float32)\n",
        "        labels = labels.to(device, dtype=torch.float32)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate MSE loss\n",
        "        mse_loss = mse_criterion(outputs, labels)\n",
        "\n",
        "        # Calculate MAE loss\n",
        "\n",
        "        # Combine MSE and MAE losses according to your requirements\n",
        "        loss = mse_loss\n",
        "\n",
        "        # Update the test losses\n",
        "        test_loss += loss.item()\n",
        "        test_mse_loss += mse_loss.item()\n",
        "\n",
        "        # Get predictions\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "\n",
        "        # Update total and correct predictions\n",
        "        total += labels.size(0)\n",
        "\n",
        "# Calculate average test losses and accuracy\n",
        "avg_test_loss = test_loss / len(testloader)\n",
        "avg_test_mse_loss = test_mse_loss / len(testloader)\n",
        "#############\n",
        "\n",
        "loss = avg_test_mse_loss\n",
        "# Step 4: Make predictions based on our test dataset\n",
        "x_test = x_test.to(device, dtype=torch.float32)\n",
        "predictions= model(x_test)\n",
        "predictions = np.array(predictions.tolist())\n",
        "\n",
        "#predictions_val = model.predict(x_validate)\n",
        "\n",
        "\n",
        "A = mpimg.imread(r'map.bmp')\n",
        "B = A[:, (np.arange(460)),:]\n",
        "\n",
        "#Figure 2\n",
        "fig = plt.figure(2, figsize=(10, 10))\n",
        "fig.suptitle('CNN Predictions Analysis')\n",
        "ax1 = plt.subplot2grid((1, 1), (0, 0), colspan=1, rowspan=1)\n",
        "# Graph the predictions against the actual values\n",
        "ax1.set_title('Comparison of predictions and actual values')\n",
        "ax1.imshow(B)\n",
        "xVec = 1000*y_test[:, 0]\n",
        "yVec = 1000*y_test[:, 1]\n",
        "xConv = B.shape[1]*(xVec+750)/1000\n",
        "yConv = B.shape[0]*(1-(yVec+1000)/1700)\n",
        "ax1.plot(xConv, yConv, 'b.', label='Actual')\n",
        "xVec = 1000*predictions[:, 0]\n",
        "yVec = 1000*predictions[:, 1]\n",
        "xConv = B.shape[1]*(xVec+750)/1000\n",
        "yConv = B.shape[0]*(1-(yVec+1000)/1700)\n",
        "ax1.plot(xConv, yConv, 'r.', label='Predicted')\n",
        "\n",
        "xVec = cells[:,1]\n",
        "yVec = cells[:,2]\n",
        "xConv = B.shape[1]*(xVec+750)/1000\n",
        "yConv = B.shape[0]*(1-(yVec+1000)/1700)\n",
        "ax1.plot(xConv, yConv, 'k*', label='Cells', markersize=10)\n",
        "ax1.grid(True, which='both', axis='both')\n",
        "ax1.legend()\n",
        "ax1.axis([-200, 800, -100, 900])\n",
        "plt.gca().invert_yaxis()\n",
        "#plt.show()\n",
        "\n",
        "#Figure 3\n",
        "fig = plt.figure(3, figsize=(10, 10))\n",
        "fig.suptitle('NN Predictions Analysis')\n",
        "ax1 = plt.subplot2grid((1, 1), (0, 0), colspan=1, rowspan=1)\n",
        "y_test = y_test.numpy()\n",
        "err = 1000*y_test-1000*predictions\n",
        "#err_val = 1000*y_validate-1000*predictions_val\n",
        "ax1.set_title('Error between real position to predictions and actual values')\n",
        "ax1.plot(err[:, 0], err[:, 1], 'b.', label='Error')\n",
        "ax1.legend()\n",
        "ax1.grid(True, which='both', axis='both')\n",
        "ax1.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "#Figure 4\n",
        "fig = plt.figure(4, figsize=(10, 10))\n",
        "err_total = (err[:,0]**2+err[:,1]**2)**0.5\n",
        "err_std_x = np.sqrt(np.mean(err[:,0]**2))\n",
        "err_std_y = np.sqrt(np.mean(err[:,1]**2))\n",
        "np.mean(np.sqrt(err[:, 0] ** 2 + err[:, 1] ** 2))\n",
        "fig.suptitle(f'Prediction errors in true locations, STD_x = {np.round(err_std_x)} , STD_y = {np.round(err_std_y)}')\n",
        "ax1 = plt.subplot2grid((1, 1), (0, 0), colspan=1, rowspan=1)\n",
        "ax1.plot(1000*y_test[err_total<100, 0], 1000*y_test[err_total<100, 1], 'b.', label='0-100 meters')\n",
        "ax1.plot(1000*y_test[(err_total>=100)&(err_total<200), 0], 1000*y_test[(err_total>=100)&(err_total<200), 1], 'g.', label='100-200meters')\n",
        "ax1.plot(1000*y_test[(err_total>=200)&(err_total<300), 0], 1000*y_test[(err_total>=200)&(err_total<300), 1], 'c.', label='200-300 meters')\n",
        "ax1.plot(1000*y_test[(err_total>=300), 0], 1000*y_test[(err_total>=300), 1], 'r.', label='300+ meters')\n",
        "ax1.plot(cells[:,1], cells[:,2], 'k*', label='Cells', markersize=10)\n",
        "ax1.legend()\n",
        "ax1.grid(True, which='both', axis='both')\n",
        "ax1.axis('equal')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}